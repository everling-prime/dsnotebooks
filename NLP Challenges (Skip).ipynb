{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "%config Application.log_level=\"INFO\"\n",
    "\n",
    "data_dir = \"/home/ubuntu/raw_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112936"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Read in Title texts\n",
    "raw_titles = [line.strip().replace(\".\", \"\").replace(\"$\", \"_$\")\n",
    "              for line in open(data_dir+\"/titles\", encoding=\"utf8\")]\n",
    "len(raw_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112936"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Read in Plot texts\n",
    "import re\n",
    "plot_boundary = re.compile(\"<EOS>\\\\n\") # \"<EOS>\\n\" separates plots\n",
    "\n",
    "raw_plots = open(data_dir+\"/plots\", encoding=\"utf8\").read()\n",
    "raw_plots = re.split(plot_boundary, raw_plots)\n",
    "del raw_plots[-1] #deletes the empty string at the end of resulting list\n",
    "len(raw_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Major, the old boar on the Manor Farm, summons the animals on the farm together for a meeting, d\n"
     ]
    }
   ],
   "source": [
    "plot_dict = dict(zip(raw_titles, raw_plots))\n",
    "print(plot_dict[\"Animal Farm\"][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP / Clustering Challenges\n",
    "\n",
    "\n",
    "##### Challenge 1\n",
    "\n",
    "Cluster sentences with K-means. If you have your own Fletcher text data, get sentences out and cluster them. If not, cluster the tweets you gathered during the last challenge set. For each cluster, print out the sentences, try to see how close the sentences are. Try different K values and try to find a K value that makes the most sense (the sentences look like they do form a meaningful cluster).\n",
    "\n",
    "How do you deal with retweets (if you're clustering tweets)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected plots: 10000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# #get sentences to cluster\n",
    "# raw_plots = open(data_dir+\"/plots\", encoding=\"utf8\").read()\n",
    "# sentences = re.split('\\n', raw_plots)\n",
    "# print(\"Number of all sentences: \" + str(len(sentences)))\n",
    "\n",
    "# ### Randomly select subsample of sentences\n",
    "# sentences = random.sample(sentences, 100000)\n",
    "# print(\"Number of selected sentences: \" + str(len(sentences)))\n",
    "\n",
    "# ### Randomly select subsample of plots\n",
    "sampled_plots = random.sample(plot_dict.items(), 10000)\n",
    "print(\"Number of selected plots: \" + str(len(sampled_plots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Companions in Nightmare',\n",
       "  \"Dr.\\nLawrence Strelson (Melvyn Douglas) is a famous psychiatrist who conducts a group-therapy session with several high-priced professionals.\\nIt turns out that one of the patients is a murderer; the truth will come out, and it will be a shocker.\\nAmong the special guest suspects are Eric Nicholson (Gig Young), Carlotta Mauridge (Anne Baxter), Jeremy Siddack (Patrick O'Neal), Julie Klanton (Dana Wynter) and dr Neesden (Leslie Nielsen).\\n\"),\n",
       " ('Monkey Kingdom',\n",
       "  \"Maya is a toque macaque whose world is changed when her son Kip becomes part of her extended family.\\nMaya’s family has its share of diverse personalities and she wishes her son to have the best advantages for advancing within the family's social strata.\\nWhen their home is overrun by a neighboring tribe of monkeys, the family has to find a new home.\\nMaya uses her inherent smarts to lead the family to new resources, but it develops that the entire group will have to cooperate in order to reclaim their original home, where Maya wishes to advance her son's future within the family.\\n\"),\n",
       " ('Treachery (film)',\n",
       "  'An estranged father and son are re-united at a wedding party.\\nHowever, a storm traps them in a cabin, leading to worse things to come.\\n'),\n",
       " ('Falling Overnight',\n",
       "  'Falling Overnight tells the story of 22-year-old Elliot Carson (Parker Croft) on the day before he has surgery to remove a brain tumor.\\nFacing what could be his last night, Elliot’s path intersects with Chloe Webb (Emilia Zoryan), a beautiful young photographer who invites him to her art show.\\nScared and alone, Elliot welcomes the distraction and as the night descends, Chloe takes him on an intimate and exhilarating journey through the city.\\nBut as morning approaches, and Chloe learns of Elliot’s condition, the magic of the evening unravels, and they must together face the uncertainty of Elliot’s future.\\n'),\n",
       " ('Courage Under Fire',\n",
       "  \"While serving in the Gulf War, Lieutenant Colonel Serling (Denzel Washington) accidentally destroys one of his own tanks during a confusing night-time battle, killing his friend Captain Boylar.\\nThe US Army covers up the details and transfers Serling to a desk job.\\nLater, Serling is assigned to determine if Captain Karen Emma Walden (Meg Ryan) should be the first woman to receive a (posthumous) Medal of Honor.\\nShe was the commander of a Medevac Huey that was sent to rescue the crew of a shot-down Black Hawk.\\nWhen she encountered a T-54, her crew destroyed it by dropping a fuel bladder onto the tank and igniting it with a flare gun.\\nHowever, her own helicopter was shot down soon after.\\nThe two crews were unable to join forces, and when the survivors were rescued the next day, Walden was reported dead.\\nSerling notices inconsistencies between the testimonies of Walden's crew.\\nSpecialist Andrew Ilario (Matt Damon), the medic, praises Walden strongly.\\nHowever, Staff Sergeant John Monfriez (Lou Diamond Phillips) claims that Walden was a coward and that he led the crew in combat and improvised the fuel bladder weapon.\\nSergeant Altameyer, who is dying in a hospital, complains about a fire.\\nWarrant Officer One Rady, the co-pilot, was injured early on and unconscious throughout.\\nFurthermore, the crew of the Black Hawk claim that they heard firing from an M16, but Ilario and Monfriez deny they had one.\\nUnder pressure from the White House and his commander, Brigadier General Hershberg (Michael Moriarty), to wrap things up quickly, Serling leaks the story to newspaper reporter Tony Gartner (Scott Glenn) to prevent another cover-up.\\nWhen Serling grills Monfriez during a car ride, Monfriez forces him to get out of the vehicle at gunpoint, then commits suicide by driving into an oncoming train.\\nSerling tracks Ilario down, and Ilario finally tells him the truth.\\nMonfriez wanted to flee, which would mean abandoning Rady.\\nWhen Walden refused, he pulled a gun on her.\\nWalden then shot an enemy who appeared behind Monfriez, but Monfriez thought she was firing at him and shot Walden in the stomach, before backing off.\\nThe next morning, the enemy attacked again as a rescue party approached.\\nWalden covered her men's retreat, firing an M16.\\nHowever, Monfriez told the rescuers that Walden was dead, so they left without her.\\nNapalm was then dropped on the entire area.\\nAltameyer tried to expose Monfriez's lie at the time, but was too injured to speak, and Ilario was too scared of the court-martial Walden had threatened them with and remained silent.\\nSerling presents his final report to Hershberg.\\nWalden's young daughter receives the Medal of Honor at a White House ceremony.\\nLater, Serling tells the truth to the Boylars about the manner of their son's death and says he cannot ask for forgiveness.\\nThe Boylars tell Serling he must put down the burden at some point.\\nIn the last moments, Serling has a flashback of when he was standing by Boylar's destroyed tank and a medevac Huey was lifting off with his friend's body.\\nSerling suddenly realises Walden was the Huey pilot.\\n\"),\n",
       " ('Man and Boy (2002 film)',\n",
       "  'After a man becomes involved in a brief affair his wife decides to leave him and he is left to bring up their young son by himself.\\n'),\n",
       " ('The Devil and Daniel Webster',\n",
       "  'Farmer Jabez Stone, from the small town of Cross Corners, New Hampshire, is plagued with unending bad luck, causing him to finally swear \"it\\'s enough to make a man want to sell his soul to the devil.\\n\" Stone is visited the next day by a stranger, who later identifies himself as \"Mr.\\nScratch,\" and makes such an offer in exchange for seven years of prosperity.\\nStone agrees.\\nAfter seven years, mr Scratch comes for Stone\\'s soul.\\nStone bargains for an additional three years; after the additional three years passes, mr Scratch refuses any further extension.\\nWanting out of the deal, Stone convinces famous lawyer and orator Daniel Webster to accept his case.\\nAt midnight of the appointed date, mr Scratch arrives and is greeted by Webster, presenting himself as Stone\\'s attorney.\\nmr Scratch tells Webster, \"I shall call upon you, as a law-abiding citizen, to assist me in taking possession of my property,\" and so begins the argument.\\nIt goes poorly for Webster, since the signature and the contract are clear, and mr Scratch will not compromise.\\nIn desperation Webster thunders, \"Mr.\\nStone is an American citizen, and no American citizen may be forced into the service of a foreign prince.\\nWe fought England for that in \\'12 and we\\'ll fight all hell for it again.\\n\" To this mr Scratch insists on his citizenship, citing his presence at the worst events of the US, concluding, \"though I don\\'t like to boast of it, my name is older in this country than yours\".\\nWebster demands a trial as the right of every American.\\nmr Scratch agrees after Webster says that he can select the judge and jury, \"so long as it is an American judge and an American jury\".\\nA jury of the damned then enters, \"with the fires of hell still upon them\".\\nThey had all done evil, and had all played a part in the formation of the United States: After five other unnamed jurors enter (Benedict Arnold being out \"on other business\"), the judge enters last &ndash; John Hathorne, the infamous and unrepentant executor of the Salem witch trials.\\nThe trial is rigged against Webster.\\nHe is ready to rage, without care for himself or Stone, but he catches himself: he sees in the jurors\\' eyes that they want him to act thus.\\nHe calms himself, \"for it was him they\\'d come for, not only Jabez Stone\".\\nWebster starts to orate on simple and good things – \"the freshness of a fine morning.\\nthe taste of food when you\\'re hungry.\\nthe new day that\\'s every day when you\\'re a child\" – and how \"without freedom, they sickened\".\\nHe speaks passionately of how wonderful it is to be human and to be an American.\\nHe admits the wrongs done in the course of American history but points out that something new and good had grown from them and that \"everybody had played a part in it, even the traitors\".\\nMankind \"got tricked and trapped and bamboozled, but it was a great journey,\" something \"no demon that was ever foaled\" could ever understand.\\nThe jury announces its verdict: \"We find for the defendant, Jabez Stone\".\\nThey admit, \"Perhaps \\'tis not strictly in accordance with the evidence, but even the damned may salute the eloquence of mr Webster\".\\nThe judge and jury disappear with the break of dawn.\\nmr Scratch congratulates Webster, and the contract is torn up.\\nWebster then grabs the stranger and twists his arm behind his back, \"for he knew that once you bested anybody like mr Scratch in fair fight, his power on you was gone\".\\nWebster makes him agree \"never to bother Jabez Stone nor his heirs or assigns nor any other New Hampshire man till doomsday.\\n\"  mr Scratch offers to tell Webster\\'s fortune in his palm.\\nHe foretells Webster\\'s failure to become President, the death of Webster\\'s sons, and the backlash of his last speech, warning \"Some will call you Ichabod\" (as in John Greenleaf Whittier\\'s poem in reaction to Webster\\'s controversial Seventh of March Speech supporting the Compromise of 1850 that incorporated The Fugitive Slave Act).\\nScratch predicts actual events of Daniel Webster\\'s life: he did have ambitions to become President, his sons died in war, and as a result of the Seventh of March Speech, many in the North considered Webster a traitor.\\nWebster takes the predictions in stride and asks only if the Union will prevail.\\nScratch reluctantly admits that although a war will be fought over the issue, the United States will remain united.\\nWebster then laughs, \"And with that he drew back his foot for a kick that would have stunned a horse.\\nIt was only the tip of his shoe that caught the stranger, but he went flying out of the door with his collecting box under his arm\".\\nIt is said that the Devil never did come back to New Hampshire.\\nThe story then ends with Jabez Stone moving from New Hampshire to North Carolina, where he found a new wife and had three children, Samantha, Alex, and Alfie.\\n'),\n",
       " ('Heathen (film)',\n",
       "  \"It has been a year since William's brother David went missing.\\nThe worst year of his life, one that has left him a broken man.\\nChloe, an attractive French artist moves into William's bleak apartment block and expresses an interest in him, bringing him an abstract portrait as a gift.\\nThey become an item and William's life appears to be taking a turn for the better.\\nThis is short lived however, as out of the blue William starts receiving strange messages relating to the disappearance of his brother.\\nHis relationship with Chloe starts to suffer as William becomes very paranoid, believing a strange man is following him and sending him the messages.\\nWilliam catches sight of the mysterious man leaving his apartment block.\\nInside he finds Chloe, deeply upset after being attacked by the man and threatened.\\nShe is instructed to give William a message: the whereabouts of his brothers body.\\nWilliam and Chloe immediately embark on a car journey to the forest location where the man has instructed them to find the grave.\\nUpon arriving, they are greeted by the mysterious man, Harry.\\nHarry reveals his involvement in David's killing and points to where the body is buried.\\nWilliam attacks Harry and demands more information.\\nHarry, bloodied and bruised from Williams punches reveals that it was not he who killed David, but Chloe.\\nWilliam, deeply shocked listens as Harry explains how Chloe seemingly accidentally asphyxiated David.\\nWilliam snaps, believing Harry to be lying to him, he launches at him once again, only to impale him accidentally on a tree stump, killing him.\\nChloe suddenly changes, revealing her true personality, and claiming that she planned for William to kill Harry all along, freeing her of the only witness to David's death.\\nAfter a short struggle with Chloe, William realises he is trapped, blackmailed into burying Harry's body and keeping Chloe's evil secrets from the police.\\n\"),\n",
       " ('The Longest Hunt',\n",
       "  \"US-American adventurer Stark has been sentenced to death in a Mexican village.\\nA rich Mexican rancher saves him from getting hanged but he must return the favour by saving the rancher's son Fidel.\\nHe is told that Fidel has somehow been persuaded or even forced to join a gang.\\nStark shall bring Fidel back to his father.\\nThe American believes that the rancher is worried sick about his son's well-being.\\nWhen he delivers Fidel he understands just on time that things are very different.\\nThe rancher intends to whitewash his honour by getting rid of Fidel because he's ashamed of having an illegitimate son.\\n\"),\n",
       " ('24 Mani Neram',\n",
       "  'Heroine Nalini gets killed by villain Sathyaraj and the hero Mohan takes oath to kill the villain within 24 hours.\\n')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_plots[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "plot_sample = raw_plots[:sample_size]\n",
    "#plot_sample = sampled_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.54 s, sys: 0 ns, total: 2.54 s\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the text data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# count_vectorizer = CountVectorizer(ngram_range=(1, 2),  \n",
    "#                                    stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "# count_vectorizer.fit(raw_plots)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "%time tfidf_vectorizer.fit(plot_sample)\n",
    "\n",
    "vectorizer = tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.17 s, sys: 0 ns, total: 1.17 s\n",
      "Wall time: 1.17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 335865)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the term-document matrix\n",
    "%time counts = vectorizer.transform(plot_sample)\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 s, sys: 304 ms, total: 25.3 s\n",
      "Wall time: 16.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=10, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "num_clusters = 10\n",
    "#km = MiniBatchKMeans(n_clusters=num_clusters, batch_size=1000)\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "%time km.fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_.shape)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dict = { 'cluster': clusters, 'title': raw_titles[:sample_size], 'plot': plot_sample }\n",
    "\n",
    "frame = pd.DataFrame(labeled_dict, index = [clusters] , columns = ['cluster', 'title', 'plot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    207\n",
       "3    163\n",
       "2    108\n",
       "5     97\n",
       "0     94\n",
       "1     92\n",
       "6     83\n",
       "9     72\n",
       "8     43\n",
       "7     41\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.head()\n",
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: b'joe', b'film', b'cole', b'max', b'vincent', b'ash',\n",
      "\n",
      "Cluster 0 titles: Army of Darkness, Citizen Kane, Destry Rides Again, Eyes Wide Shut, Evil Dead II, Fearless (1993 film), Hapworth 16, 1924, Johnny Got His Gun, Mulholland Drive (film), Meet the Feebles,\n",
      "\n",
      "Cluster 1 words: b'george', b'carrie', b'billy', b'mr', b'julie', b'father',\n",
      "\n",
      "Cluster 1 titles: Anyone Can Whistle, A Funny Thing Happened on the Way to the Forum, Carousel (musical), Four Weddings and a Funeral, My Fair Lady, The Spanish Inquisition (Monty Python), My Neighbor Totoro, Original Sin (2001 film), Pride and Prejudice, Raging Bull,\n",
      "\n",
      "Cluster 2 words: b'mary', b'fred', b'man', b'al', b'home', b'father',\n",
      "\n",
      "Cluster 2 titles: A Clockwork Orange (novel), Blade Runner, Blue Velvet (film), Barry Lyndon, Bank of China Tower (Hong Kong), The Cider House Rules, Day of the Tentacle, Death of a Hero, Escape from New York, The Trial,\n",
      "\n",
      "Cluster 3 words: b'earth', b'planet', b'king', b'ship', b'game', b'moon',\n",
      "\n",
      "Cluster 3 titles: A Fire Upon the Deep, Braveheart, Bubblegum Crisis, Castle of the Winds, Chrono Trigger, Carmilla, Chrono Cross, Doraemon, Doom (1993 video game), Diablo II,\n",
      "\n",
      "Cluster 4 words: b'island', b'john', b'richard', b'new', b'life', b'time',\n",
      "\n",
      "Cluster 4 titles: Animal Farm, The Plague, The Big O, Bildungsroman, Crash (J G Ballard novel), Cowboy Bebop, Dracula, Das Boot, The Evil Dead, Young and Innocent,\n",
      "\n",
      "Cluster 5 words: b'godzilla', b'japanese', b'war', b'attack', b'soviet', b'crew',\n",
      "\n",
      "Cluster 5 titles: Babylon 5, Chariots of Fire, Dressed to Kill (1980 film), Dr Strangelove, King Kong vs Godzilla, Godzilla vs Biollante, Terror of Mechagodzilla, Godzilla vs King Ghidorah, Godzilla (1954 film), The Return of Godzilla,\n",
      "\n",
      "Cluster 6 words: b'jane', b'anne', b'family', b'mother', b'henry', b'dave',\n",
      "\n",
      "Cluster 6 titles: Blade Runner 2: The Edge of Human, Crouching Tiger, Hidden Dragon, The Metamorphosis, Monty Python's Life of Brian, Dead Parrot sketch, Miss Congeniality (film), Nanook of the North, Stuart Little, The Shining (novel), Soldier of Fortune (video game),\n",
      "\n",
      "Cluster 7 words: b'sam', b'batman', b'harvey', b'max', b'frodo', b'bruce',\n",
      "\n",
      "Cluster 7 titles: Batman (1989 film), Batman (1966 film), Batman Returns, Batman &amp; Robin (film), Batman Forever, Batman: Year One, Bride of the Monster, Psycho (1960 film), Preacher (comics), The Lord of the Rings,\n",
      "\n",
      "Cluster 8 words: b'paul', b'tony', b'leto', b'bene', b'gesserit', b'bene',\n",
      "\n",
      "Cluster 8 titles: All Quiet on the Western Front, Buffy the Vampire Slayer (film), Chapterhouse: Dune, Vladimir Harkonnen, Dune Messiah, Formant, God Emperor of Dune, Heretics of Dune, Orgy of the Dead, Plotter,\n",
      "\n",
      "Cluster 9 words: b'ben', b'jesus', b'tom', b'dr', b'cameron', b'men',\n",
      "\n",
      "Cluster 9 titles: Actaeon, The Birth of a Nation, Blazing Saddles, The Pinchcliffe Grand Prix, Four Feather Falls, Follies, Gone with the Wind (novel), Left Behind, One Foot in the Grave, Princess Mononoke,\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "vocab_frame = pd.DataFrame(terms, index=terms)\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.loc[i]['title'].values.tolist()[:10]:\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge 2\n",
    "\n",
    "Draw the inertia curve over different k values. (The sklearn KMeans class has an inertia_ attribute.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcdbc4fb2b0>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAVcQ9P9MYJZWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcdbc472908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_clusters = 10\n",
    "\n",
    "inertias = []\n",
    "for n in range(1, num_clusters):\n",
    "    #km = MiniBatchKMeans(n_clusters=n, batch_size=1000)\n",
    "    km = KMeans(n_clusters=n)\n",
    "    km.fit(counts)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "plt.plot(range(1,num_clusters), inertias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[992.6212648855328,\n",
       " 991.21869488378,\n",
       " 989.7242778167448,\n",
       " 988.3884498853116,\n",
       " 987.1765644102122,\n",
       " 986.0588534792674,\n",
       " 984.4722935272273,\n",
       " 983.314262518235,\n",
       " 981.9310119146555]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inertias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge 3\n",
    "\n",
    "__Let's name the clusters 1__\n",
    "\n",
    "For each cluster, find the sentence closest to the centroid of the cluster.\n",
    "\n",
    "(You can use [sklearn.metrics.pairwise_distances](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances) or [scipy.spatial.distance](http://docs.scipy.org/doc/scipy/reference/spatial.distance.html) [check `pdist`, `cdist`, and `euclidean distance`] to find distances to the centroid). KMeans has a `cluster_centers_` attribute.\n",
    "\n",
    "This sentence (closest to centroid) is now the name of the cluster. For each cluster, print the representative sentence, and print 'N people expressed a similar statement', or something like that relevant to your dataset. (This is very close to what amazon used to do in the reviews section.)\n",
    "\n",
    "Find the biggest 3 clusters, and print their representative sentences. (This is close to what amazon is doing now in the reviews section, except they choose the sentence from the most helpful review instead of closest to center.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top titles per cluster:\n",
      "\n",
      "Cluster 0 titles: Johnny Got His Gun, Neuromancer, The Big Lebowski, Midnight Cowboy, Everyone Says I Love You,\n",
      "\n",
      "Cluster 1 titles: Animal Farm, A Clockwork Orange (novel), The Plague, Actaeon, A Fire Upon the Deep,\n",
      "\n",
      "Cluster 2 titles: Citizen Kane, Show Me Love (film), The Spanish Inquisition (Monty Python), The Parent Trap (1961 film), How Green Was My Valley,\n",
      "\n",
      "Cluster 3 titles: Army of Darkness, Blazing Saddles, Blue Velvet (film), Batman (1989 film), Batman (1966 film),\n",
      "\n",
      "Cluster 4 titles: Doraemon, Heretic II, Icehenge, Return to Castle Wolfenstein, Sailor Moon,\n",
      "\n",
      "Cluster 5 titles: Bank of China Tower (Hong Kong), The Trial, Fearless (1993 film), Adventures of Huckleberry Finn, Into the Woods,\n",
      "\n",
      "Cluster 6 titles: Carousel (musical), Gaudy Night, Miss Congeniality (film), Pride and Prejudice, The Rocky Horror Picture Show,\n",
      "\n",
      "Cluster 7 titles: Blade Runner, Blade Runner 2: The Edge of Human, Carmilla, Dressed to Kill (1980 film), Mulholland Drive (film),\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 8\n",
    "km = MiniBatchKMeans(n_clusters=num_clusters, batch_size=1000)\n",
    "km.fit(counts)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "labeled_dict = { 'cluster': clusters, 'title': raw_titles[:sample_size], 'plot': plot_sample }\n",
    "\n",
    "frame = pd.DataFrame(labeled_dict, index = [clusters] , columns = ['cluster', 'title', 'plot'])\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Top titles per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "vocab_frame = pd.DataFrame(terms, index=terms)\n",
    "for i in range(num_clusters):\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.loc[i]['title'].values.tolist()[:5]:\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    632\n",
       "3    106\n",
       "5     78\n",
       "6     52\n",
       "4     47\n",
       "0     39\n",
       "2     25\n",
       "7     21\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "0    Joe Bonham, a young American soldier serving i...\n",
       "1    Old Major, the old boar on the Manor Farm, sum...\n",
       "2    In a mansion in Xanadu, a vast palatial estate...\n",
       "3    Being transported to the Middle Ages, Ash Will...\n",
       "4    Nobita is a young boy who suffers from poor gr...\n",
       "5    The site on which the building is constructed ...\n",
       "6    Two young female millworkers in 1873 Maine vis...\n",
       "7    In Los Angeles in November 2019, ex-police off...\n",
       "Name: plot, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_cluster = frame['plot'].groupby(frame['cluster'])\n",
    "\n",
    "cluster_concat_df = groupby_cluster.apply(lambda x: \" \".join(x))\n",
    "cluster_concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge 4\n",
    "\n",
    "__Let's name the clusters 2__\n",
    "\n",
    "Calculate the tf-idf of each word in each cluster (think of all sentences of a cluster together as a document). Represent each cluster with the top 1, or top 2 or... to 5 tf-idf words. For each cluster, print the name (keywords) of the cluster, and \"N statements\" in the cluster (N is the size of the cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "cluster_tfidf_vectorizer.fit(cluster_concat_df)\n",
    "\n",
    "vectorizer = cluster_tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)\n",
    "\n",
    "\"\"\"\n",
    "tf(word, blob) computes \"term frequency\" which is the number of times a word appears in a document blob, normalized by dividing by the total number of words in blob. We use TextBlob for breaking up the text into words and getting the word counts.\n",
    "\n",
    "n_containing(word, bloblist) returns the number of documents containing word. A generator expression is passed to the sum() function.\n",
    "\n",
    "idf(word, bloblist) computes \"inverse document frequency\" which measures how common a word is among all documents in bloblist. The more common a word is, the lower its idf. We take the ratio of the total number of documents to the number of documents containing word, then take the log of that. Add 1 to the divisor to prevent division by zero.\n",
    "\n",
    "tfidf(word, blob, bloblist) computes the TF-IDF score. It is simply the product of tf and idf.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: Dude, TF-IDF: 0.00291\n",
      "\tWord: Case, TF-IDF: 0.00213\n",
      "\tWord: McCauley, TF-IDF: 0.00183\n",
      "Top words in document 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-233c73c5c52e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbloblist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top words in document {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbloblist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msorted_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-233c73c5c52e>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbloblist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top words in document {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbloblist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msorted_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-259047f705ba>\u001b[0m in \u001b[0;36mtfidf\u001b[0;34m(word, blob, bloblist)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbloblist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbloblist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-132-259047f705ba>\u001b[0m in \u001b[0;36mtf\u001b[0;34m(word, blob)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mn_containing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbloblist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self, strg, case_sensitive, *args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcase_sensitive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             return [word.lower() for word in self].count(strg.lower(), *args,\n\u001b[0m\u001b[1;32m    248\u001b[0m                     **kwargs)\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcase_sensitive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             return [word.lower() for word in self].count(strg.lower(), *args,\n\u001b[0m\u001b[1;32m    248\u001b[0m                     **kwargs)\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bloblist = cluster_concat_df.apply(tb)\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in cluster {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge 5\n",
    "\n",
    "__Let's name the clusters 3__\n",
    "\n",
    "Same as the previous challenge, but this time, calculate tf-idf only for nouns (NN tag) and build keyword(s) with nouns. (This is close to what amazon switched to last year, before settling into the current design). (They would show five nouns, you would click on one and it would show sentences - linked to the reviews- that were related to that noun.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge 6\n",
    "\n",
    "Cluster the same data with [MiniBatchKMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html). MiniBatchKMeans is a fast way to apply K-means to large data without much loss -- The results are very similar. Instead of using EVERY single point to find the new place of the centroid, MiniBatch just randomly samples a small number (like 100) in the cluster to calculate the new center. Since this is usually very close to the actual center, the algorithm gets there much faster. Try it and compare the results. ([Example on two-feature data](http://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge 7\n",
    "\n",
    "Switch the __init__ parameter to \"random\" (instead of the default kmeans++) and plot the inertia curve for each of the __n_init__ values for K-Means: 1, 2, 3, 10 (n_init is the number of different runs to try with different random initializations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge 8\n",
    "\n",
    "Download [this dataset on the purchase stats from clients of a wholesale distributor](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers). Cluster the clients based on their annual spending features (fresh, milk, grocery, frozen, detergents_paper, delicatessen). Remember to scale the features before clustering. After finding a reasonable amount of clusters, for EACH cluster, plot the histogram for every single feature: FRESH, MILK, GROCERY, FROZEN, DETERGENTS_PAPER, DELICATESSEN, CHANNEL, REGION. Is there a natural way to characterize each cluster? How would you describe each cluster to the wholesale distributor if you were working for them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Extensions (Optional)\n",
    "\n",
    "Use the same code for your previous clustering challenges.\n",
    "\n",
    "Repeat each challenge (except the inertia curves, since only the KMeans implementation gives a quick way of calculating that.) However, this time, try (both) Agglomerative Clustering and DBSCAN instead of KMeans.\n",
    "\n",
    "For text clustering, use cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
